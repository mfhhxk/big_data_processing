{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming application using Spark Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Write code to create a SparkSession, which uses four cores with a proper application name, use the Melbourne timezone, and make sure a checkpoint location has been set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext # Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import os\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.0.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"A2B_Task2\") \\\n",
    "    .config(\"spark.master\", \"local[4]\")\\\n",
    "    .config(\"spark.sql.session.timeZone\", \"Australia/Melbourne\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Similar to assignment 2A, write code to define the data schema for the data files, following the data types suggested in the metadata file. Load the static datasets (e.g. customer, product, category) into data frames. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StringType, StructField,DateType, DoubleType, IntegerType, TimestampType\n",
    "\n",
    "# static datasets\n",
    "cat_schema = StructType([\n",
    "    StructField(\"#\", StringType(), True),\n",
    "    # uniquie identifier - not nominal \n",
    "    StructField(\"category_id\", StringType(), True),\n",
    "    \n",
    "    # hierachical level but the number will be not used to do maths operations\n",
    "    StructField(\"cat_level1\", StringType(), True),\n",
    "    StructField(\"cat_level2\", StringType(), True),\n",
    "    StructField(\"cat_level3\", StringType(), True)\n",
    "])\n",
    "\n",
    "cust_schema = StructType([\n",
    "    StructField(\"#\", StringType(), True),\n",
    "     StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"username\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    # date - dob of the customer\n",
    "    StructField(\"birthdate\", DateType(), True),\n",
    "    StructField(\"device_type\", StringType(), True),\n",
    "    StructField(\"device_id\", StringType(), True),\n",
    "    StructField(\"device_version\", StringType(), True),\n",
    "    # double for long- and lat-itude\n",
    "    StructField(\"home_location_lat\", DoubleType(), True),\n",
    "    StructField(\"home_location_long\", DoubleType(), True),\n",
    "    StructField(\"home_location\", StringType(), True),\n",
    "    StructField(\"home_country\", StringType(), True),\n",
    "    # date - date when the customer first joined or registered\n",
    "    StructField(\"first_join_date\", DateType(), True) \n",
    "])\n",
    "\n",
    "prod_schema = StructType([\n",
    "    StructField(\"#\", StringType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"baseColour\", StringType(), True),\n",
    "    StructField(\"season\", StringType(), True),\n",
    "    # year - product was introduced or became available. used for sorting or filtering \n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"usage\", StringType(), True),\n",
    "    StructField(\"productDisplayName\", StringType(), True),\n",
    "    StructField(\"category_id\", StringType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### cat_df schema:\n",
      "Number of partitions: 1\n",
      "root\n",
      " |-- category_id: string (nullable = true)\n",
      " |-- cat_level1: string (nullable = true)\n",
      " |-- cat_level2: string (nullable = true)\n",
      " |-- cat_level3: string (nullable = true)\n",
      "\n",
      "####### cust_df schema:\n",
      "Number of partitions: 4\n",
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- username: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- birthdate: date (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- device_id: string (nullable = true)\n",
      " |-- device_version: string (nullable = true)\n",
      " |-- home_location_lat: double (nullable = true)\n",
      " |-- home_location_long: double (nullable = true)\n",
      " |-- home_location: string (nullable = true)\n",
      " |-- home_country: string (nullable = true)\n",
      " |-- first_join_date: date (nullable = true)\n",
      "\n",
      "####### prod_df schema:\n",
      "Number of partitions: 1\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- baseColour: string (nullable = true)\n",
      " |-- season: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- usage: string (nullable = true)\n",
      " |-- productDisplayName: string (nullable = true)\n",
      " |-- category_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# category\n",
    "cat_df = spark.read.csv('a2a_dataset/category.csv',\n",
    "                        header=True,schema = cat_schema)\n",
    "cat_df = cat_df.drop(cat_df[0])\n",
    "print(f\"####### cat_df schema:\")\n",
    "print(f\"Number of partitions: {cat_df.rdd.getNumPartitions()}\")\n",
    "cat_df.printSchema()\n",
    "\n",
    "\n",
    "# customer\n",
    "cust_df = spark.read.csv('a2a_dataset/customer.csv',\n",
    "                         header = True, schema = cust_schema)\n",
    "cust_df = cust_df.drop(cust_df[0])\n",
    "print(f\"####### cust_df schema:\")\n",
    "print(f\"Number of partitions: {cust_df.rdd.getNumPartitions()}\")\n",
    "cust_df.printSchema()\n",
    "\n",
    "# product\n",
    "prod_df = spark.read.csv('a2a_dataset/product.csv', \n",
    "                         header=True, schema = prod_schema)\n",
    "prod_df = prod_df.drop(prod_df[0])\n",
    "print(f\"####### prod_df schema:\")\n",
    "print(f\"Number of partitions: {prod_df.rdd.getNumPartitions()}\")\n",
    "prod_df.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     StructField(\"#\", StringType(), True),\n",
    "#     StructField(\"traffic_source\", StringType(), True),\n",
    "#     StructField(\"event_metadata\", StringType(), True),\n",
    "# clickstream_schema\n",
    "click_schema = StructType([\n",
    "    StructField(\"session_id\", StringType(), True),\n",
    "    StructField(\"event_name\", StringType(), True),\n",
    "    StructField(\"event_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"event_metadata\", StringType(), True),\n",
    "    StructField(\"ts\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3 Using the Kafka topic from the producer in Task 1, ingest the streaming data into Spark Streaming, assuming all data comes in the String format. Except for the 'ts' column, you shall receive it as an Int type.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# topic from Task1\n",
    "topic = 'clickstream_realtime'\n",
    "\n",
    "#configuration\n",
    "hostip = \"118.139.61.182\" #change me\n",
    "\n",
    "\n",
    "# connect to producer \n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", f'{hostip}:9092') \\\n",
    "    .option(\"startingOffsets\", \"latest\")\\\n",
    "    .option(\"subscribe\", topic) \\\n",
    "    .load()\n",
    "\n",
    "df = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Then, the streaming data format should be transformed into the proper formats following the metadata file schema, similar to assignment 2A.  \n",
    "Perform the following tasks:  \n",
    "a) For the 'ts' column, convert it to the timestamp format, we will use it as event_time.  \n",
    "b) If the data is late for more than 1 minute, discard it.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- parsed: struct (nullable = true)\n",
      " |    |-- session_id: string (nullable = true)\n",
      " |    |-- event_name: string (nullable = true)\n",
      " |    |-- event_id: string (nullable = true)\n",
      " |    |-- customer_id: string (nullable = true)\n",
      " |    |-- event_metadata: string (nullable = true)\n",
      " |    |-- ts: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.select(F.from_json(F.col(\"value\").cast(\"string\"), \n",
    "                           click_schema).alias('parsed'))\n",
    "\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- event_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- event_metadata: string (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# #,session_id,event_name,event_id,traffic_source,event_metadata,customer_id\n",
    "#                     F.col(\"parsed_value.traffic_source\").alias(\"traffic_source\"),\n",
    "#                     F.col(\"parsed_value.event_metadata\").alias(\"event_metadata\"),\n",
    "df_formatted = df.select(\n",
    "                    F.col(\"parsed.session_id\").alias(\"session_id\"),\n",
    "                    F.col(\"parsed.event_name\").alias(\"event_name\"),\n",
    "                    F.col(\"parsed.event_id\").alias(\"event_id\"),\n",
    "                    F.col(\"parsed.customer_id\").alias(\"customer_id\"),\n",
    "                    F.col(\"parsed.event_metadata\").alias(\"event_metadata\"),\n",
    "                    F.col(\"parsed.ts\").cast(\"timestamp\").alias(\"event_time\")\n",
    "                ).withWatermark(\"event_time\", \"2 minute\")\n",
    "df_formatted.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5  \n",
    "Aggregate the streaming data frame by session id and create features you used in your assignment 2A model. (note: customer ID has already been included in the stream.)   \n",
    "Then, join the static data frames with the streaming data frame as our final data for prediction.  \n",
    "Perform data type/column conversion according to your ML model, and print out the Schema.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selected_df in A2A  \n",
    "root  \n",
    " |-- highval%: double (nullable = true)  \n",
    " |-- num_cat_midvalue: long (nullable = true)  \n",
    " |-- season: string (nullable = true)  \n",
    " |-- gender: string (nullable = true)  \n",
    " |-- device_type: string (nullable = true)  \n",
    " |-- is_promotion: boolean (nullable = true)  \n",
    " |-- label: integer (nullable = false)  \n",
    " |-- months_since_joining: double (nullable = true)  \n",
    "\n",
    "\n",
    "is_promotion is defined by event_name == \"ADD_PROMO\"  \n",
    "\n",
    "Features relevant to click_stream data: highval%, num_cat_midvalue, is_promotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- event_metadata: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- first_join_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined = df_formatted\\\n",
    "    .join(cust_df,'customer_id','inner')\\\n",
    "    .select('session_id','customer_id', 'event_name','event_time','event_metadata','gender','device_type','first_join_date')\\\n",
    "    .withWatermark(\"event_time\", \"1 minute\")\n",
    "df_joined.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf,col\n",
    "\n",
    "# month to season\n",
    "def season(month):\n",
    "    if month in [3, 4, 5]:\n",
    "        return\"Spring\"\n",
    "    elif month in [6, 7, 8]:\n",
    "        return \"Summer\"\n",
    "    elif month in [9,10,11]:\n",
    "        return \"Autumn\"\n",
    "    else:\n",
    "        return \"Winter\"\n",
    "\n",
    "season_udf = udf(season, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- event_metadata: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- first_join_date: date (nullable = true)\n",
      " |-- season: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract the month of each event time with aliases\n",
    "month_df = df_joined\\\n",
    "    .withColumn(\"event_month\", F.month(df_joined[\"event_time\"])) \\\n",
    "    .withColumn(\"season\", season_udf(col(\"event_month\"))) \\\n",
    "    .drop(\"event_month\")\n",
    "month_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_df_query= month_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"month_df\") \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-----------+-------------------+--------------------+------+-----------+---------------+------+\n",
      "|          session_id|customer_id| event_name|         event_time|      event_metadata|gender|device_type|first_join_date|season|\n",
      "+--------------------+-----------+-----------+-------------------+--------------------+------+-----------+---------------+------+\n",
      "|7aba64ce-8f48-491...|      76356|      CLICK|2023-10-08 18:23:33|                    |     F|    Android|     2021-07-03|Autumn|\n",
      "|7aba64ce-8f48-491...|      76356|     SCROLL|2023-10-08 18:23:33|                    |     F|    Android|     2021-07-03|Autumn|\n",
      "|7aba64ce-8f48-491...|      76356|     SEARCH|2023-10-08 18:23:33|{'search_keywords...|     F|    Android|     2021-07-03|Autumn|\n",
      "|7aba64ce-8f48-491...|      76356|      CLICK|2023-10-08 18:23:33|                    |     F|    Android|     2021-07-03|Autumn|\n",
      "|7aba64ce-8f48-491...|      76356|      CLICK|2023-10-08 18:23:33|                    |     F|    Android|     2021-07-03|Autumn|\n",
      "|7aba64ce-8f48-491...|      76356|     SCROLL|2023-10-08 18:23:33|                    |     F|    Android|     2021-07-03|Autumn|\n",
      "|7aba64ce-8f48-491...|      76356|      CLICK|2023-10-08 18:23:33|                    |     F|    Android|     2021-07-03|Autumn|\n",
      "|7aba64ce-8f48-491...|      76356|      CLICK|2023-10-08 18:23:33|                    |     F|    Android|     2021-07-03|Autumn|\n",
      "|7aba64ce-8f48-491...|      76356|   PURCHASE|2023-10-08 18:23:33|{'payment_status'...|     F|    Android|     2021-07-03|Autumn|\n",
      "|7aba64ce-8f48-491...|      76356| VIEW_PROMO|2023-10-08 18:23:33|                    |     F|    Android|     2021-07-03|Autumn|\n",
      "|7aba64ce-8f48-491...|      76356|     SCROLL|2023-10-08 18:23:33|                    |     F|    Android|     2021-07-03|Autumn|\n",
      "|7aba64ce-8f48-491...|      76356|     SCROLL|2023-10-08 18:23:33|                    |     F|    Android|     2021-07-03|Autumn|\n",
      "|7aba64ce-8f48-491...|      76356|   HOMEPAGE|2023-10-08 18:23:33|                    |     F|    Android|     2021-07-03|Autumn|\n",
      "|7aba64ce-8f48-491...|      76356|     SCROLL|2023-10-08 18:23:33|                    |     F|    Android|     2021-07-03|Autumn|\n",
      "|7aba64ce-8f48-491...|      76356|      CLICK|2023-10-08 18:23:33|                    |     F|    Android|     2021-07-03|Autumn|\n",
      "|7aba64ce-8f48-491...|      76356|     SCROLL|2023-10-08 18:23:33|                    |     F|    Android|     2021-07-03|Autumn|\n",
      "|7aba64ce-8f48-491...|      76356|   HOMEPAGE|2023-10-08 18:23:33|                    |     F|    Android|     2021-07-03|Autumn|\n",
      "|7aba64ce-8f48-491...|      76356|ADD_TO_CART|2023-10-08 18:23:33|{'product_id': 41...|     F|    Android|     2021-07-03|Autumn|\n",
      "|7abc3390-0da5-48b...|      55262|ADD_TO_CART|2023-10-08 18:23:33|{'product_id': 26...|     M|    Android|     2019-05-19|Autumn|\n",
      "|7abc3390-0da5-48b...|      55262|   HOMEPAGE|2023-10-08 18:23:33|                    |     M|    Android|     2019-05-19|Autumn|\n",
      "+--------------------+-----------+-----------+-------------------+--------------------+------+-----------+---------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from month_df\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_df_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- num_cat_highvalue: long (nullable = true)\n",
      " |-- num_cat_midvalue: long (nullable = true)\n",
      " |-- num_cat_lowvalue: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import window ,when\n",
    "\n",
    "# grouped by session_id, and window\n",
    "# considering windown time: 1min\n",
    "# w/o watermark: not coming late\n",
    "categories_df = df_formatted \\\n",
    "    .groupBy(\"session_id\", window(df_formatted.event_time, \"1 minute\"))\\\n",
    "    .agg(\n",
    "        F.sum(when(df_formatted[\"event_name\"].isin(['ADD_PROMO', 'ADD_TO_CART']), 1).otherwise(0)).alias(\"num_cat_highvalue\"),\n",
    "        F.sum(when(df_formatted[\"event_name\"].isin(['VIEW_PROMO', 'VIEW_ITEM', 'SEARCH']), 1).otherwise(0)).alias(\"num_cat_midvalue\"),\n",
    "        F.sum(when(df_formatted[\"event_name\"].isin(['SCROLL', 'HOMEPAGE', 'CLICK']), 1).otherwise(0)).alias(\"num_cat_lowvalue\"),\n",
    "    )\n",
    "\n",
    "\n",
    "# Show the schema of categories_df\n",
    "categories_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- is_promotion: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "promotion_df = df_formatted \\\n",
    "    .groupBy(\"session_id\", window(df_formatted.event_time, \"1 minute\"))\\\n",
    "    .agg(\n",
    "        (F.max(F.when(F.col(\"event_name\") == \"ADD_PROMO\", 1).otherwise(0)) == 1).alias(\"is_promotion\")\n",
    ")\n",
    "promotion_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- event_metadata: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- first_join_date: date (nullable = true)\n",
      " |-- season: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- is_promotion: boolean (nullable = true)\n",
      "\n",
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- event_metadata: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- first_join_date: date (nullable = true)\n",
      " |-- season: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "month_df.printSchema()\n",
    "promotion_df.printSchema()\n",
    "month_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- event_metadata: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- first_join_date: date (nullable = true)\n",
      " |-- season: string (nullable = true)\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- num_cat_highvalue: long (nullable = true)\n",
      " |-- num_cat_midvalue: long (nullable = true)\n",
      " |-- num_cat_lowvalue: long (nullable = true)\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- is_promotion: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined = month_df\\\n",
    ".select(\"session_id\", \"customer_id\",\"event_name\",\"event_time\",\"event_metadata\",\n",
    "        \"gender\",\"device_type\",\"first_join_date\",\"season\")\\\n",
    ".join(categories_df,'session_id','inner')\\\n",
    ".join(promotion_df.select(\"session_id\", \"window\", \"is_promotion\"),'session_id','inner')\n",
    "\n",
    "df_joined.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- event_metadata: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- first_join_date: date (nullable = true)\n",
      " |-- season: string (nullable = true)\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- num_cat_highvalue: long (nullable = true)\n",
      " |-- num_cat_midvalue: long (nullable = true)\n",
      " |-- num_cat_lowvalue: long (nullable = true)\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- is_promotion: boolean (nullable = true)\n",
      " |-- highval%: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round\n",
    "total_actions = df_joined.num_cat_highvalue + df_joined.num_cat_midvalue + df_joined.num_cat_lowvalue\n",
    "\n",
    "ratio_df = df_joined.withColumn('highval%', \n",
    "                                round(df_joined.num_cat_highvalue / total_actions*100,2))\n",
    "ratio_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- event_metadata: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- season: string (nullable = true)\n",
      " |-- num_cat_midvalue: long (nullable = true)\n",
      " |-- is_promotion: boolean (nullable = true)\n",
      " |-- highval%: double (nullable = true)\n",
      " |-- months_since_joining: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import months_between, current_date\n",
    "\n",
    "feature_df = ratio_df.withColumn(\n",
    "    \"months_since_joining\",\n",
    "    months_between(current_date(), \"first_join_date\")).drop(\"first_join_date\")\\\n",
    "    .select(\"session_id\", \"customer_id\",'event_name',\"event_time\",'event_metadata',\"gender\", \"device_type\", \"season\", \"num_cat_midvalue\", \n",
    "            \"is_promotion\",\"highval%\",\"months_since_joining\")\n",
    "feature_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_df_query= feature_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"feature_df\") \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+-----------+-----------+-------------------+----------------------------------------------------------+------+-----------+------+----------------+------------+--------+--------------------+\n",
      "|session_id                          |customer_id|event_name |event_time         |event_metadata                                            |gender|device_type|season|num_cat_midvalue|is_promotion|highval%|months_since_joining|\n",
      "+------------------------------------+-----------+-----------+-------------------+----------------------------------------------------------+------+-----------+------+----------------+------------+--------+--------------------+\n",
      "|803c7826-ff62-4c82-aee1-9039686efba7|48765      |CLICK      |2023-10-08 18:24:57|                                                          |F     |Android    |Autumn|1               |false       |0.0     |63.22580645         |\n",
      "|803c7826-ff62-4c82-aee1-9039686efba7|48765      |SEARCH     |2023-10-08 18:24:57|{'search_keywords': 'Celana Panjang'}                     |F     |Android    |Autumn|1               |false       |0.0     |63.22580645         |\n",
      "|7ffcee7a-4724-4320-a7e5-39fd9b0be061|41681      |ADD_TO_CART|2023-10-08 18:24:57|{'product_id': 57286, 'quantity': 1, 'item_price': 232323}|F     |Android    |Autumn|0               |false       |85.71   |54.61290323         |\n",
      "|803ab738-4542-4ac0-96e4-7feefb4d4ec0|5223       |HOMEPAGE   |2023-10-08 18:24:57|                                                          |F     |iOS        |Autumn|0               |false       |33.33   |66.32258065         |\n",
      "|803ab738-4542-4ac0-96e4-7feefb4d4ec0|5223       |ADD_TO_CART|2023-10-08 18:24:57|{'product_id': 23091, 'quantity': 1, 'item_price': 210740}|F     |iOS        |Autumn|0               |false       |33.33   |66.32258065         |\n",
      "|803ab738-4542-4ac0-96e4-7feefb4d4ec0|5223       |PURCHASE   |2023-10-08 18:24:57|{'payment_status': 'Success'}                             |F     |iOS        |Autumn|0               |false       |33.33   |66.32258065         |\n",
      "|803ab738-4542-4ac0-96e4-7feefb4d4ec0|5223       |HOMEPAGE   |2023-10-08 18:24:57|                                                          |F     |iOS        |Autumn|0               |false       |33.33   |66.32258065         |\n",
      "|8021166a-7c06-4da7-8ec8-bc86d3c02091|51084      |CLICK      |2023-10-08 18:24:57|                                                          |F     |Android    |Autumn|2               |false       |46.15   |26.51612903         |\n",
      "|8021166a-7c06-4da7-8ec8-bc86d3c02091|51084      |SCROLL     |2023-10-08 18:24:57|                                                          |F     |Android    |Autumn|2               |false       |46.15   |26.51612903         |\n",
      "|8021166a-7c06-4da7-8ec8-bc86d3c02091|51084      |SCROLL     |2023-10-08 18:24:57|                                                          |F     |Android    |Autumn|2               |false       |46.15   |26.51612903         |\n",
      "|8021166a-7c06-4da7-8ec8-bc86d3c02091|51084      |ADD_TO_CART|2023-10-08 18:24:57|{'product_id': 55648, 'quantity': 1, 'item_price': 436695}|F     |Android    |Autumn|2               |false       |46.15   |26.51612903         |\n",
      "|8021166a-7c06-4da7-8ec8-bc86d3c02091|51084      |CLICK      |2023-10-08 18:24:57|                                                          |F     |Android    |Autumn|2               |false       |46.15   |26.51612903         |\n",
      "|8021166a-7c06-4da7-8ec8-bc86d3c02091|51084      |SEARCH     |2023-10-08 18:24:57|{'search_keywords': 'Second Hand'}                        |F     |Android    |Autumn|2               |false       |46.15   |26.51612903         |\n",
      "|8021166a-7c06-4da7-8ec8-bc86d3c02091|51084      |ADD_TO_CART|2023-10-08 18:24:57|{'product_id': 27400, 'quantity': 1, 'item_price': 236738}|F     |Android    |Autumn|2               |false       |46.15   |26.51612903         |\n",
      "|8021166a-7c06-4da7-8ec8-bc86d3c02091|51084      |ADD_TO_CART|2023-10-08 18:24:57|{'product_id': 10718, 'quantity': 3, 'item_price': 265985}|F     |Android    |Autumn|2               |false       |46.15   |26.51612903         |\n",
      "|8021166a-7c06-4da7-8ec8-bc86d3c02091|51084      |HOMEPAGE   |2023-10-08 18:24:57|                                                          |F     |Android    |Autumn|2               |false       |46.15   |26.51612903         |\n",
      "|8021166a-7c06-4da7-8ec8-bc86d3c02091|51084      |PURCHASE   |2023-10-08 18:24:57|{'payment_status': 'Failed'}                              |F     |Android    |Autumn|2               |false       |46.15   |26.51612903         |\n",
      "|8021166a-7c06-4da7-8ec8-bc86d3c02091|51084      |ADD_TO_CART|2023-10-08 18:24:57|{'product_id': 14461, 'quantity': 1, 'item_price': 340121}|F     |Android    |Autumn|2               |false       |46.15   |26.51612903         |\n",
      "|8021166a-7c06-4da7-8ec8-bc86d3c02091|51084      |ADD_TO_CART|2023-10-08 18:24:57|{'product_id': 27728, 'quantity': 1, 'item_price': 244414}|F     |Android    |Autumn|2               |false       |46.15   |26.51612903         |\n",
      "|8021166a-7c06-4da7-8ec8-bc86d3c02091|51084      |ADD_TO_CART|2023-10-08 18:24:57|{'product_id': 8216, 'quantity': 2, 'item_price': 462560} |F     |Android    |Autumn|2               |false       |46.15   |26.51612903         |\n",
      "+------------------------------------+-----------+-----------+-------------------+----------------------------------------------------------+------+-----------+------+----------------+------------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from feature_df\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- event_metadata: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- season: string (nullable = true)\n",
      " |-- num_cat_midvalue: long (nullable = true)\n",
      " |-- is_promotion: boolean (nullable = true)\n",
      " |-- highval%: double (nullable = true)\n",
      " |-- months_since_joining: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above feature_df match with the targeted schema for feature_df:\n",
    "\n",
    "root  \n",
    "|-- highval%: double (nullable = true)  \n",
    "|-- num_cat_midvalue: long (nullable = true)  \n",
    "|-- season: string (nullable = true)  \n",
    "|-- gender: string (nullable = true)  \n",
    "|-- device_type: string (nullable = true)  \n",
    "|-- is_promotion: boolean (nullable = true)  \n",
    "|-- label: integer (nullable = false)  \n",
    "|-- months_since_joining: double (nullable = true)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 Load your ML model, and use the model to predict if each session will purchase according to the requirements below:\n",
    "a) Every 10 seconds, show the total number of potential sales transactions (prediction = 1) in the last 1 minute.   \n",
    "b) Every 30 seconds, show the total potential revenue in the last 30 seconds. “Potiential revenue” here is definded as: When prediction=1, extract customer shopping cart detail from metadata (sum of all items of ADD_TO_CART events).  \n",
    "c) Every 1 minute, show the top 10 best-selling products by total quantity. (note: No historical data is required, only the top 10 in each 1 minute window.)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\tGBTClassifier_37f2b33bc82e-featuresCol: features,\n",
      "\tGBTClassifier_37f2b33bc82e-labelCol: label\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#Loading the Pipeline Model From the filesystem\n",
    "from pyspark.ml import PipelineModel\n",
    "pipelineModel = PipelineModel.load('asg2a_prediction_model')\n",
    "print(pipelineModel.stages[-1]._java_obj.paramMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 6a\n",
    "## Fit the pipeline to new data\n",
    "# gbt\n",
    "gbt_predictions = pipelineModel.transform(feature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_predictions_query = gbt_predictions \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"gbt_predictions\")\\\n",
    "    .trigger(processingTime=\"5 seconds\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+-----------+-----------+-------------------+----------------------------------------------------------+------+-----------+------+----------------+------------+--------+--------------------+------------+--------------+------------+--------------+-----------------+-------------------+-----------------------------------+------------------------------------------+-----------------------------------------+----------+\n",
      "|session_id                          |customer_id|event_name |event_time         |event_metadata                                            |gender|device_type|season|num_cat_midvalue|is_promotion|highval%|months_since_joining|season_index|season_encoded|gender_index|gender_encoded|device_type_index|device_type_encoded|features                           |rawPrediction                             |probability                              |prediction|\n",
      "+------------------------------------+-----------+-----------+-------------------+----------------------------------------------------------+------+-----------+------+----------------+------------+--------+--------------------+------------+--------------+------------+--------------+-----------------+-------------------+-----------------------------------+------------------------------------------+-----------------------------------------+----------+\n",
      "|7ade3f01-1205-4316-a892-62453c8714ca|98001      |HOMEPAGE   |2023-10-08 20:19:55|                                                          |F     |Android    |Autumn|1               |false       |33.33   |36.19354839         |3.0         |(3,[],[])     |0.0         |(1,[0],[1.0]) |0.0              |(1,[0],[1.0])      |(8,[0,1,5,6],[33.33,1.0,1.0,1.0])  |[-0.21491226763961493,0.21491226763961493]|[0.3941682317016517,0.6058317682983483]  |1.0       |\n",
      "|7ade3f01-1205-4316-a892-62453c8714ca|98001      |ADD_TO_CART|2023-10-08 20:19:55|{'product_id': 26808, 'quantity': 1, 'item_price': 137907}|F     |Android    |Autumn|1               |false       |33.33   |36.19354839         |3.0         |(3,[],[])     |0.0         |(1,[0],[1.0]) |0.0              |(1,[0],[1.0])      |(8,[0,1,5,6],[33.33,1.0,1.0,1.0])  |[-0.21491226763961493,0.21491226763961493]|[0.3941682317016517,0.6058317682983483]  |1.0       |\n",
      "|7ade3f01-1205-4316-a892-62453c8714ca|98001      |SEARCH     |2023-10-08 20:19:55|{'search_keywords': 'Bekas'}                              |F     |Android    |Autumn|1               |false       |33.33   |36.19354839         |3.0         |(3,[],[])     |0.0         |(1,[0],[1.0]) |0.0              |(1,[0],[1.0])      |(8,[0,1,5,6],[33.33,1.0,1.0,1.0])  |[-0.21491226763961493,0.21491226763961493]|[0.3941682317016517,0.6058317682983483]  |1.0       |\n",
      "|7ade3f01-1205-4316-a892-62453c8714ca|98001      |PURCHASE   |2023-10-08 20:19:55|{'payment_status': 'Success'}                             |F     |Android    |Autumn|1               |false       |33.33   |36.19354839         |3.0         |(3,[],[])     |0.0         |(1,[0],[1.0]) |0.0              |(1,[0],[1.0])      |(8,[0,1,5,6],[33.33,1.0,1.0,1.0])  |[-0.21491226763961493,0.21491226763961493]|[0.3941682317016517,0.6058317682983483]  |1.0       |\n",
      "|fd4c5e27-d91b-42f8-a04d-d99828b1f81c|49693      |ADD_TO_CART|2023-10-08 20:19:47|{'product_id': 15852, 'quantity': 1, 'item_price': 333136}|M     |iOS        |Autumn|3               |false       |30.0    |60.80645161         |3.0         |(3,[],[])     |1.0         |(1,[],[])     |1.0              |(1,[],[])          |(8,[0,1],[30.0,3.0])               |[1.5432977795806928,-1.5432977795806928]  |[0.9563364262326434,0.043663573767356634]|0.0       |\n",
      "|fd4c5e27-d91b-42f8-a04d-d99828b1f81c|49693      |HOMEPAGE   |2023-10-08 20:19:47|                                                          |M     |iOS        |Autumn|3               |false       |30.0    |60.80645161         |3.0         |(3,[],[])     |1.0         |(1,[],[])     |1.0              |(1,[],[])          |(8,[0,1],[30.0,3.0])               |[1.5432977795806928,-1.5432977795806928]  |[0.9563364262326434,0.043663573767356634]|0.0       |\n",
      "|fd4c5e27-d91b-42f8-a04d-d99828b1f81c|49693      |ADD_TO_CART|2023-10-08 20:19:47|{'product_id': 30291, 'quantity': 1, 'item_price': 208079}|M     |iOS        |Autumn|3               |false       |30.0    |60.80645161         |3.0         |(3,[],[])     |1.0         |(1,[],[])     |1.0              |(1,[],[])          |(8,[0,1],[30.0,3.0])               |[1.5432977795806928,-1.5432977795806928]  |[0.9563364262326434,0.043663573767356634]|0.0       |\n",
      "|fd4c5e27-d91b-42f8-a04d-d99828b1f81c|49693      |HOMEPAGE   |2023-10-08 20:19:47|                                                          |M     |iOS        |Autumn|3               |false       |30.0    |60.80645161         |3.0         |(3,[],[])     |1.0         |(1,[],[])     |1.0              |(1,[],[])          |(8,[0,1],[30.0,3.0])               |[1.5432977795806928,-1.5432977795806928]  |[0.9563364262326434,0.043663573767356634]|0.0       |\n",
      "|fd4c5e27-d91b-42f8-a04d-d99828b1f81c|49693      |VIEW_ITEM  |2023-10-08 20:19:47|                                                          |M     |iOS        |Autumn|3               |false       |30.0    |60.80645161         |3.0         |(3,[],[])     |1.0         |(1,[],[])     |1.0              |(1,[],[])          |(8,[0,1],[30.0,3.0])               |[1.5432977795806928,-1.5432977795806928]  |[0.9563364262326434,0.043663573767356634]|0.0       |\n",
      "|fd4c5e27-d91b-42f8-a04d-d99828b1f81c|49693      |PURCHASE   |2023-10-08 20:19:47|{'payment_status': 'Success'}                             |M     |iOS        |Autumn|3               |false       |30.0    |60.80645161         |3.0         |(3,[],[])     |1.0         |(1,[],[])     |1.0              |(1,[],[])          |(8,[0,1],[30.0,3.0])               |[1.5432977795806928,-1.5432977795806928]  |[0.9563364262326434,0.043663573767356634]|0.0       |\n",
      "|fd4c5e27-d91b-42f8-a04d-d99828b1f81c|49693      |CLICK      |2023-10-08 20:19:47|                                                          |M     |iOS        |Autumn|3               |false       |30.0    |60.80645161         |3.0         |(3,[],[])     |1.0         |(1,[],[])     |1.0              |(1,[],[])          |(8,[0,1],[30.0,3.0])               |[1.5432977795806928,-1.5432977795806928]  |[0.9563364262326434,0.043663573767356634]|0.0       |\n",
      "|fd4c5e27-d91b-42f8-a04d-d99828b1f81c|49693      |SCROLL     |2023-10-08 20:19:47|                                                          |M     |iOS        |Autumn|3               |false       |30.0    |60.80645161         |3.0         |(3,[],[])     |1.0         |(1,[],[])     |1.0              |(1,[],[])          |(8,[0,1],[30.0,3.0])               |[1.5432977795806928,-1.5432977795806928]  |[0.9563364262326434,0.043663573767356634]|0.0       |\n",
      "|fd4c5e27-d91b-42f8-a04d-d99828b1f81c|49693      |SEARCH     |2023-10-08 20:19:47|{'search_keywords': 'Sepatu Nike'}                        |M     |iOS        |Autumn|3               |false       |30.0    |60.80645161         |3.0         |(3,[],[])     |1.0         |(1,[],[])     |1.0              |(1,[],[])          |(8,[0,1],[30.0,3.0])               |[1.5432977795806928,-1.5432977795806928]  |[0.9563364262326434,0.043663573767356634]|0.0       |\n",
      "|fd4c5e27-d91b-42f8-a04d-d99828b1f81c|49693      |SEARCH     |2023-10-08 20:19:47|{'search_keywords': 'Baju'}                               |M     |iOS        |Autumn|3               |false       |30.0    |60.80645161         |3.0         |(3,[],[])     |1.0         |(1,[],[])     |1.0              |(1,[],[])          |(8,[0,1],[30.0,3.0])               |[1.5432977795806928,-1.5432977795806928]  |[0.9563364262326434,0.043663573767356634]|0.0       |\n",
      "|fd4c5e27-d91b-42f8-a04d-d99828b1f81c|49693      |ADD_TO_CART|2023-10-08 20:19:47|{'product_id': 11422, 'quantity': 1, 'item_price': 163207}|M     |iOS        |Autumn|3               |false       |30.0    |60.80645161         |3.0         |(3,[],[])     |1.0         |(1,[],[])     |1.0              |(1,[],[])          |(8,[0,1],[30.0,3.0])               |[1.5432977795806928,-1.5432977795806928]  |[0.9563364262326434,0.043663573767356634]|0.0       |\n",
      "|339978cd-9218-41c2-be13-ca5010a1298f|88680      |SEARCH     |2023-10-08 20:19:41|{'search_keywords': 'Dress Pesta'}                        |F     |Android    |Autumn|5               |true        |45.45   |65.58064516         |3.0         |(3,[],[])     |0.0         |(1,[0],[1.0]) |0.0              |(1,[0],[1.0])      |[45.45,5.0,0.0,0.0,0.0,1.0,1.0,1.0]|[-1.3943963431126143,1.3943963431126143]  |[0.0579328112018152,0.9420671887981849]  |1.0       |\n",
      "|339978cd-9218-41c2-be13-ca5010a1298f|88680      |VIEW_PROMO |2023-10-08 20:19:41|                                                          |F     |Android    |Autumn|5               |true        |45.45   |65.58064516         |3.0         |(3,[],[])     |0.0         |(1,[0],[1.0]) |0.0              |(1,[0],[1.0])      |[45.45,5.0,0.0,0.0,0.0,1.0,1.0,1.0]|[-1.3943963431126143,1.3943963431126143]  |[0.0579328112018152,0.9420671887981849]  |1.0       |\n",
      "|339978cd-9218-41c2-be13-ca5010a1298f|88680      |ADD_PROMO  |2023-10-08 20:19:41|{'promo_code': 'BUYMORE', 'promo_amount': 3261}           |F     |Android    |Autumn|5               |true        |45.45   |65.58064516         |3.0         |(3,[],[])     |0.0         |(1,[0],[1.0]) |0.0              |(1,[0],[1.0])      |[45.45,5.0,0.0,0.0,0.0,1.0,1.0,1.0]|[-1.3943963431126143,1.3943963431126143]  |[0.0579328112018152,0.9420671887981849]  |1.0       |\n",
      "|339978cd-9218-41c2-be13-ca5010a1298f|88680      |ADD_TO_CART|2023-10-08 20:19:41|{'product_id': 42034, 'quantity': 1, 'item_price': 593136}|F     |Android    |Autumn|5               |true        |45.45   |65.58064516         |3.0         |(3,[],[])     |0.0         |(1,[0],[1.0]) |0.0              |(1,[0],[1.0])      |[45.45,5.0,0.0,0.0,0.0,1.0,1.0,1.0]|[-1.3943963431126143,1.3943963431126143]  |[0.0579328112018152,0.9420671887981849]  |1.0       |\n",
      "|339978cd-9218-41c2-be13-ca5010a1298f|88680      |VIEW_ITEM  |2023-10-08 20:19:41|                                                          |F     |Android    |Autumn|5               |true        |45.45   |65.58064516         |3.0         |(3,[],[])     |0.0         |(1,[0],[1.0]) |0.0              |(1,[0],[1.0])      |[45.45,5.0,0.0,0.0,0.0,1.0,1.0,1.0]|[-1.3943963431126143,1.3943963431126143]  |[0.0579328112018152,0.9420671887981849]  |1.0       |\n",
      "+------------------------------------+-----------+-----------+-------------------+----------------------------------------------------------+------+-----------+------+----------------+------------+--------+--------------------+------------+--------------+------------+--------------+-----------------+-------------------+-----------------------------------+------------------------------------------+-----------------------------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from gbt_predictions\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_predictions_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_sales= gbt_predictions\\\n",
    ".groupBy(F.col('prediction'), window(\"event_time\", \"1 minute\"))\\\n",
    ".agg(\n",
    "    F.count(F.when(gbt_predictions.prediction == 1, 1).otherwise(0)).alias(\"Tt_sales\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tt_sales_query = tt_sales \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"tt_sales\")\\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------------------------------+--------+\n",
      "|prediction|window                                    |Tt_sales|\n",
      "+----------+------------------------------------------+--------+\n",
      "|0.0       |{2023-10-08 18:44:00, 2023-10-08 18:45:00}|947     |\n",
      "|1.0       |{2023-10-08 18:44:00, 2023-10-08 18:45:00}|3413    |\n",
      "|0.0       |{2023-10-08 18:46:00, 2023-10-08 18:47:00}|1715    |\n",
      "|1.0       |{2023-10-08 18:45:00, 2023-10-08 18:46:00}|3219    |\n",
      "|1.0       |{2023-10-08 18:46:00, 2023-10-08 18:47:00}|6399    |\n",
      "|0.0       |{2023-10-08 18:45:00, 2023-10-08 18:46:00}|1013    |\n",
      "+----------+------------------------------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from tt_sales\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_sales_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- event_metadata: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- season: string (nullable = true)\n",
      " |-- num_cat_midvalue: long (nullable = true)\n",
      " |-- is_promotion: boolean (nullable = true)\n",
      " |-- highval%: double (nullable = true)\n",
      " |-- months_since_joining: double (nullable = true)\n",
      " |-- season_index: double (nullable = false)\n",
      " |-- season_encoded: vector (nullable = true)\n",
      " |-- gender_index: double (nullable = false)\n",
      " |-- gender_encoded: vector (nullable = true)\n",
      " |-- device_type_index: double (nullable = false)\n",
      " |-- device_type_encoded: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6b\n",
    "gbt_predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "# metadata is read as a string\n",
    "# to extract item_price to cal rev -- parse the struct\n",
    "metadata_schema = StructType([\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"item_price\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# condition: prediction=1 & add_to_cart\n",
    "# potential rev = qty * item_price\n",
    "potential_revenue = gbt_predictions \\\n",
    "    .filter((F.col('prediction') == 1) & (F.col('event_name') == 'ADD_TO_CART')) \\\n",
    "    .withColumn(\"metadata_struct\", from_json(F.col(\"event_metadata\"), metadata_schema)) \\\n",
    "    .groupBy(window(F.col('event_time'), '30 seconds')) \\\n",
    "    .agg(\n",
    "        F.sum(F.when(F.col('prediction') == 1,\n",
    "                     F.col(\"metadata_struct.item_price\")* F.col(\"metadata_struct.quantity\")).otherwise(0)).alias(\"potential_revenue\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_revenue_query = potential_revenue \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"potential_revenue\")\\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+-----------------+\n",
      "|window                                    |potential_revenue|\n",
      "+------------------------------------------+-----------------+\n",
      "|{2023-10-08 20:38:30, 2023-10-08 20:39:00}|1.04399511E8     |\n",
      "|{2023-10-08 20:38:00, 2023-10-08 20:38:30}|7.0977936E7      |\n",
      "|{2023-10-08 20:39:30, 2023-10-08 20:40:00}|2.28494711E8     |\n",
      "|{2023-10-08 20:39:00, 2023-10-08 20:39:30}|4.8130296E7      |\n",
      "+------------------------------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from potential_revenue').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_revenue_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6c\n",
    "from pyspark.sql.functions import from_json,rank\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "# metadata is read as a string\n",
    "# to extract item_price to cal rev -- parse the struct\n",
    "metadata_schema = StructType([\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"item_price\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# cond: prediction of purchase = 1\n",
    "# gp by time window and prod_id\n",
    "# sum over the qty to find tt_qty\n",
    "top_10 = gbt_predictions \\\n",
    "    .filter(F.col('prediction') == 1) \\\n",
    "    .withColumn(\"metadata_struct\", from_json(F.col(\"event_metadata\"), metadata_schema)) \\\n",
    "    .groupBy(\n",
    "            window(F.col('event_time'), \"1 minute\"), \n",
    "            F.col(\"metadata_struct.product_id\").alias(\"product_id\")\n",
    ") \\\n",
    "    .agg(F.sum(F.col(\"metadata_struct.quantity\")).alias(\"tt_quantity\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_query = top_10 \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"top_10\")\\\n",
    "    .trigger(processingTime=\"1 minute\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|product_id|tt_quantity|\n",
      "+----------+-----------+\n",
      "|4889      |36         |\n",
      "|39618     |20         |\n",
      "|9772      |20         |\n",
      "|47088     |17         |\n",
      "|16496     |16         |\n",
      "|48094     |16         |\n",
      "|48038     |14         |\n",
      "|25062     |13         |\n",
      "|52142     |13         |\n",
      "|1920      |12         |\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select product_id, tt_quantity from top_10 order by tt_quantity desc limit 10\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7  \n",
    "a) Persist the prediction result along with cart metadata in parquet format; after that, read the parquet file and show the results to verify it is saved properly.  \n",
    "b) Persist the 30-second sales prediction in another parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- event_metadata: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- season: string (nullable = true)\n",
      " |-- num_cat_midvalue: long (nullable = true)\n",
      " |-- is_promotion: boolean (nullable = true)\n",
      " |-- highval%: double (nullable = true)\n",
      " |-- months_since_joining: double (nullable = true)\n",
      " |-- season_index: double (nullable = false)\n",
      " |-- season_encoded: vector (nullable = true)\n",
      " |-- gender_index: double (nullable = false)\n",
      " |-- gender_encoded: vector (nullable = true)\n",
      " |-- device_type_index: double (nullable = false)\n",
      " |-- device_type_encoded: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gbt_predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- event_metadata: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- season: string (nullable = true)\n",
      " |-- num_cat_midvalue: long (nullable = true)\n",
      " |-- is_promotion: boolean (nullable = true)\n",
      " |-- highval%: double (nullable = true)\n",
      " |-- months_since_joining: double (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_save = gbt_predictions.select('session_id','customer_id','event_name','event_time',\n",
    "                                  'event_metadata', 'gender', 'device_type',\n",
    "                                   'season', 'num_cat_midvalue', 'is_promotion',\n",
    "                                   'highval%', 'months_since_joining','prediction')\n",
    "pred_save.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7a\n",
    "parquet_pred = pred_save\\\n",
    "        .writeStream\\\n",
    "        .format(\"parquet\")\\\n",
    "        .outputMode(\"append\")\\\n",
    "        .option(\"path\", \"parquet/clickstream_df/prediction\")\\\n",
    "        .option(\"checkpointLocation\", \"parquet/clickstream_df/prediction/checkpoint\")\\\n",
    "        .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- event_metadata: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- season: string (nullable = true)\n",
      " |-- num_cat_midvalue: long (nullable = true)\n",
      " |-- is_promotion: boolean (nullable = true)\n",
      " |-- highval%: double (nullable = true)\n",
      " |-- months_since_joining: double (nullable = true)\n",
      " |-- prediction: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, BooleanType, DoubleType, LongType\n",
    "parquet_pred_schema = StructType([\n",
    "    StructField(\"session_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"event_name\", StringType(), True),\n",
    "    StructField(\"event_time\", TimestampType(), True),\n",
    "    StructField(\"event_metadata\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"device_type\", StringType(), True),\n",
    "    StructField(\"season\", StringType(), True),\n",
    "    StructField(\"num_cat_midvalue\", LongType(), True),\n",
    "    StructField(\"is_promotion\", BooleanType(), True),\n",
    "    StructField(\"highval%\", DoubleType(), True),\n",
    "    StructField(\"months_since_joining\", DoubleType(), True),\n",
    "    StructField(\"prediction\", DoubleType(), False)\n",
    "])\n",
    "\n",
    "\n",
    "# Read the saved parquet data\n",
    "parquet_pred = spark.readStream.schema(parquet_pred_schema).parquet(\"parquet/clickstream_df/prediction\")\n",
    "parquet_pred.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_pred.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_pred_query = parquet_pred\\\n",
    "    .writeStream\\\n",
    "    .queryName(\"parquet_pred_df\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .format(\"memory\")\\\n",
    "    .trigger(processingTime=\"10 seconds\")\\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+-----------+-----------+-------------------+----------------------------------------------------------+------+-----------+------+----------------+------------+--------+--------------------+----------+\n",
      "|session_id                          |customer_id|event_name |event_time         |event_metadata                                            |gender|device_type|season|num_cat_midvalue|is_promotion|highval%|months_since_joining|prediction|\n",
      "+------------------------------------+-----------+-----------+-------------------+----------------------------------------------------------+------+-----------+------+----------------+------------+--------+--------------------+----------+\n",
      "|608fdb2c-20f6-498d-bc47-d373b31f4a01|78079      |SCROLL     |2023-10-08 19:57:02|                                                          |M     |Android    |Autumn|1               |false       |25.0    |40.06451613         |0.0       |\n",
      "|605f8bdd-2976-44d5-a62e-28a1f174f3fe|26248      |ADD_TO_CART|2023-10-08 19:57:02|{'product_id': 29108, 'quantity': 4, 'item_price': 149627}|M     |Android    |Autumn|2               |true        |14.29   |77.48387097         |1.0       |\n",
      "|608fdb2c-20f6-498d-bc47-d373b31f4a01|78079      |SCROLL     |2023-10-08 19:57:00|                                                          |M     |Android    |Autumn|1               |false       |25.0    |40.06451613         |0.0       |\n",
      "|608fdb2c-20f6-498d-bc47-d373b31f4a01|78079      |PURCHASE   |2023-10-08 19:57:00|{'payment_status': 'Success'}                             |M     |Android    |Autumn|1               |false       |25.0    |40.06451613         |0.0       |\n",
      "|608fdb2c-20f6-498d-bc47-d373b31f4a01|78079      |HOMEPAGE   |2023-10-08 19:57:00|                                                          |M     |Android    |Autumn|1               |false       |25.0    |40.06451613         |0.0       |\n",
      "|608fdb2c-20f6-498d-bc47-d373b31f4a01|78079      |HOMEPAGE   |2023-10-08 19:57:00|                                                          |M     |Android    |Autumn|1               |false       |25.0    |40.06451613         |0.0       |\n",
      "|608fdb2c-20f6-498d-bc47-d373b31f4a01|78079      |HOMEPAGE   |2023-10-08 19:57:00|                                                          |M     |Android    |Autumn|1               |false       |25.0    |40.06451613         |0.0       |\n",
      "|608fdb2c-20f6-498d-bc47-d373b31f4a01|78079      |HOMEPAGE   |2023-10-08 19:57:00|                                                          |M     |Android    |Autumn|1               |false       |25.0    |40.06451613         |0.0       |\n",
      "|608fdb2c-20f6-498d-bc47-d373b31f4a01|78079      |CLICK      |2023-10-08 19:57:00|                                                          |M     |Android    |Autumn|1               |false       |25.0    |40.06451613         |0.0       |\n",
      "|608fdb2c-20f6-498d-bc47-d373b31f4a01|78079      |HOMEPAGE   |2023-10-08 19:57:00|                                                          |M     |Android    |Autumn|1               |false       |25.0    |40.06451613         |0.0       |\n",
      "|608fdb2c-20f6-498d-bc47-d373b31f4a01|78079      |VIEW_ITEM  |2023-10-08 19:57:00|                                                          |M     |Android    |Autumn|1               |false       |25.0    |40.06451613         |0.0       |\n",
      "|608fdb2c-20f6-498d-bc47-d373b31f4a01|78079      |ADD_TO_CART|2023-10-08 19:57:00|{'product_id': 31330, 'quantity': 1, 'item_price': 446005}|M     |Android    |Autumn|1               |false       |25.0    |40.06451613         |0.0       |\n",
      "|608fdb2c-20f6-498d-bc47-d373b31f4a01|78079      |CLICK      |2023-10-08 19:57:00|                                                          |M     |Android    |Autumn|1               |false       |25.0    |40.06451613         |0.0       |\n",
      "|608fdb2c-20f6-498d-bc47-d373b31f4a01|78079      |ADD_TO_CART|2023-10-08 19:57:00|{'product_id': 18672, 'quantity': 1, 'item_price': 133580}|M     |Android    |Autumn|1               |false       |25.0    |40.06451613         |0.0       |\n",
      "|606e87c1-2e84-4ca9-9765-2c72ca9d3767|15461      |VIEW_ITEM  |2023-10-08 19:56:59|                                                          |M     |Android    |Autumn|4               |false       |12.5    |71.5483871          |0.0       |\n",
      "|605f8bdd-2976-44d5-a62e-28a1f174f3fe|26248      |VIEW_PROMO |2023-10-08 19:56:59|                                                          |M     |Android    |Autumn|2               |true        |14.29   |77.48387097         |1.0       |\n",
      "|6063983c-36a1-4640-b2b7-8a711015f0a1|70931      |ADD_TO_CART|2023-10-08 19:56:59|{'product_id': 58504, 'quantity': 1, 'item_price': 364461}|F     |Android    |Autumn|0               |true        |37.5    |86.51612903         |1.0       |\n",
      "|607c3a11-8d30-4c76-ab8f-259330b60bdf|83524      |PURCHASE   |2023-10-08 19:56:59|{'payment_status': 'Success'}                             |M     |Android    |Autumn|1               |false       |28.57   |81.22580645         |0.0       |\n",
      "|606e87c1-2e84-4ca9-9765-2c72ca9d3767|15461      |HOMEPAGE   |2023-10-08 19:56:59|                                                          |M     |Android    |Autumn|4               |false       |12.5    |71.5483871          |0.0       |\n",
      "|605f8bdd-2976-44d5-a62e-28a1f174f3fe|26248      |ADD_PROMO  |2023-10-08 19:56:59|{'promo_code': 'WEEKENDSERU', 'promo_amount': 9720}       |M     |Android    |Autumn|2               |true        |14.29   |77.48387097         |1.0       |\n",
      "+------------------------------------+-----------+-----------+-------------------+----------------------------------------------------------+------+-----------+------+----------------+------------+--------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM parquet_pred_df ORDER BY event_time DESC\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_pred_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- potential_revenue: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7b\n",
    "potential_revenue.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_saved = potential_revenue\\\n",
    "        .writeStream\\\n",
    "        .format(\"parquet\")\\\n",
    "        .outputMode(\"append\")\\\n",
    "        .option(\"path\", \"parquet/clickstream_df/sales\")\\\n",
    "        .option(\"checkpointLocation\", \"parquet/clickstream_df/sales/checkpoint\")\\\n",
    "        .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- window: struct (nullable = true)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- potential_revenue: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, TimestampType, DoubleType\n",
    "\n",
    "sales_parquet_schema = StructType([\n",
    "    StructField(\"window\", StructType([\n",
    "        StructField(\"start\", TimestampType(), nullable=True),\n",
    "        StructField(\"end\", TimestampType(), nullable=True)\n",
    "    ]), nullable=False),\n",
    "    StructField(\"potential_revenue\", DoubleType(), nullable=True)\n",
    "])\n",
    "\n",
    "\n",
    "# Read the saved parquet data\n",
    "sales_parquet= spark.readStream.schema(sales_parquet_schema).parquet(\"parquet/clickstream_df/sales\")\n",
    "sales_parquet.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_parquet.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_parquet_query = sales_parquet\\\n",
    "    .writeStream\\\n",
    "    .queryName(\"sales_parquet_df\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .format(\"memory\")\\\n",
    "    .trigger(processingTime=\"30 seconds\")\\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+-----------------+\n",
      "|window                                    |potential_revenue|\n",
      "+------------------------------------------+-----------------+\n",
      "|{2023-10-08 20:47:00, 2023-10-08 20:47:30}|147791.0         |\n",
      "|{2023-10-08 20:46:30, 2023-10-08 20:47:00}|1.12027758E8     |\n",
      "|{2023-10-08 20:45:30, 2023-10-08 20:46:00}|2.3073782E7      |\n",
      "|{2023-10-08 20:44:30, 2023-10-08 20:45:00}|1.30211004E8     |\n",
      "|{2023-10-08 20:44:00, 2023-10-08 20:44:30}|5.2950039E7      |\n",
      "+------------------------------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM sales_parquet_df ORDER BY window DESC\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_parquet_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8  \n",
    "Read the parquet files as a data stream, for 7a) join customer information and send to a Kafka topic with an appropriate name to the data visualisation. For 7b) Send the message directly to another Kafka topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- username: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- birthdate: date (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- device_id: string (nullable = true)\n",
      " |-- device_version: string (nullable = true)\n",
      " |-- home_location_lat: double (nullable = true)\n",
      " |-- home_location_long: double (nullable = true)\n",
      " |-- home_location: string (nullable = true)\n",
      " |-- home_country: string (nullable = true)\n",
      " |-- first_join_date: date (nullable = true)\n",
      "\n",
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- event_metadata: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- season: string (nullable = true)\n",
      " |-- num_cat_midvalue: long (nullable = true)\n",
      " |-- is_promotion: boolean (nullable = true)\n",
      " |-- highval%: double (nullable = true)\n",
      " |-- months_since_joining: double (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cust_df.printSchema()\n",
    "parquet_pred_saved.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- home_location: string (nullable = true)\n",
      " |-- home_location_lat: double (nullable = true)\n",
      " |-- home_location_long: double (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- cumulative_orders: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stream 1\n",
    "cust_pred = parquet_pred.join(cust_df, 'customer_id', 'inner')\\\n",
    "    .select(\"customer_id\", \"prediction\", \"home_location\", 'home_location_lat', \n",
    "            'home_location_long',\"event_time\")\\\n",
    "    .groupBy(\"home_location\", 'home_location_lat', \n",
    "            'home_location_long', \"event_time\")\\\n",
    "    .agg({\"prediction\": \"sum\"})\n",
    "\n",
    "aggregated_df = cust_pred.withColumnRenamed(\"sum(prediction)\", \"cumulative_orders\")\\\n",
    "    .withWatermark((\"event_time\"), \"1 minute\")  \n",
    "\n",
    "aggregated_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_df.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is being sent to Kafka topic: cumulative_orders\n"
     ]
    }
   ],
   "source": [
    "from kafka3 import KafkaProducer\n",
    "from pyspark.sql.functions import to_json, struct\n",
    "\n",
    "#configuration\n",
    "hostip = \"118.139.61.182\" #change me\n",
    "topic1 = \"cumulative_orders\"\n",
    "\n",
    "\n",
    "\n",
    "sd_to_Kafka = aggregated_df\\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", f'{hostip}:9092') \\\n",
    "    .option(\"topic\", topic) \\\n",
    "    .option(\"checkpointLocation\", \"parquet/clickstream_df/prediction/checkpoint\")\\\n",
    "    .start()\n",
    "\n",
    "\n",
    "try:\n",
    "    while not sd_to_Kafka.isActive:\n",
    "        pass\n",
    "    print(\"Data is being sent to Kafka topic:\", topic)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Producer interrupted\")\n",
    "    \n",
    "# sd_to_Kafka.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd_to_Kafka.isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- window: struct (nullable = true)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- potential_revenue: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_parquet.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "ename": "StreamingQueryException",
     "evalue": "[STREAM_FAILED] Query [id = b5d3f539-b8ff-4252-87dd-f03b0a890872, runId = 6e143ea9-6c81-4f52-a1ff-f6ddb40008c7] terminated with exception: Failed to read log file file:/home/student/parquet/clickstream_df/sales/checkpoint/sources/0/0. Log file was malformed: failed to read correct log version from \u0000v1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[301], line 15\u001b[0m\n\u001b[1;32m      6\u001b[0m sd_to_Kafka2 \u001b[38;5;241m=\u001b[39m sales_parquet\u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpointLocation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparquet/clickstream_df/sales/checkpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 15\u001b[0m     \u001b[43msd_to_Kafka2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData is being sent to Kafka topic:\u001b[39m\u001b[38;5;124m\"\u001b[39m, topic)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/sql/streaming/query.py:201\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: [STREAM_FAILED] Query [id = b5d3f539-b8ff-4252-87dd-f03b0a890872, runId = 6e143ea9-6c81-4f52-a1ff-f6ddb40008c7] terminated with exception: Failed to read log file file:/home/student/parquet/clickstream_df/sales/checkpoint/sources/0/0. Log file was malformed: failed to read correct log version from \u0000v1."
     ]
    }
   ],
   "source": [
    "# Stream 2\n",
    "hostip = \"118.139.61.182\" #change me\n",
    "topic2 = \"tt_sales\"\n",
    "\n",
    "\n",
    "sd_to_Kafka2 = sales_parquet.writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", f'{hostip}:9092') \\\n",
    "    .option(\"topic\", topic) \\\n",
    "    .option(\"checkpointLocation\", \"parquet/clickstream_df/sales/checkpoint\")\\\n",
    "    .start()\n",
    "\n",
    "try:\n",
    "    while not sd_to_Kafka2.isActive:\n",
    "        pass\n",
    "    print(\"Data is being sent to Kafka topic:\", topic)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Producer interrupted\")\n",
    "    \n",
    "# sd_to_Kafka2.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd_to_Kafka2.isActive"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
